diff --git a/.gitignore b/.gitignore
index 86de3fa..4d067fe 100644
--- a/.gitignore
+++ b/.gitignore
@@ -183,5 +183,12 @@ output.txt
 # Ignore all files in the logs directory
 logs/
 
+# Ignore all files in the logs directory
+logs_serper/
+
+# Ignore all files in the logs directory
+log_new/
+
+
 # Ignore all files in the logs directory
 result/
\ No newline at end of file
diff --git a/README.md b/README.md
index aa8ae29..b8f1e53 100644
--- a/README.md
+++ b/README.md
@@ -46,10 +46,13 @@ deep_research_py/
 uv tool install deep-research-py && cp .env.example .env
 ```
 
+### Optional: Install Playwright (only needed for `playwright_*` scraper options)
 ```bash
 playwright install
 ```
 
+**Note**: If you're using `serper_only` (recommended), you don't need to install Playwright.
+
 ### Docker with OpenWebUI
 
 ![alt text](./openwebui.png)
@@ -72,16 +75,33 @@ Open `.env` and replace placeholder values with your actual API keys
 
 ### Set up environment variables in .env file:
 ```bash
-# Required: OpenAI API key
-# unless you're using DeepSeek or another OpenAI-compliant API.
-OPENAI_API_KEY=your-openai-key-here
+# Required: GEMINI API key (unless using DeepSeek or another provider)
+GEMINI_API_KEY=your-openai-key-here
+
+# Scraper Configuration - Choose one of the following options:
+DEFAULT_SCRAPER="serper_only"  # Recommended for simplicity
 
-# Required: Firecrawl API key
+# Option 1: Serper.dev only (Recommended)
+# Uses Serper.dev for both search and scraping - fastest and most reliable
+SERPER_API_KEY=your-serper-key-here
+
+# Option 2: Firecrawl (Alternative)
+# Uses Firecrawl for both search and scraping
 FIRECRAWL_API_KEY=your-firecrawl-key-here
-# If you want to use your self-hosted Firecrawl, add the following below:
-# FIRECRAWL_BASE_URL="http://localhost:3002"
+# FIRECRAWL_BASE_URL="http://localhost:3002"  # If self-hosting
+
+# Option 3: Mixed approaches (Advanced)
+# DEFAULT_SCRAPER="playwright_serper"  # Serper search + Playwright scraping
+# DEFAULT_SCRAPER="playwright_ddgs"    # DuckDuckGo search + Playwright scraping
 ```
 
+### Scraper Options Explained:
+
+- **`serper_only`** (Recommended): Uses Serper.dev for both search and scraping. Fast, reliable, and requires only a Serper.dev API key.
+- **`firecrawl`**: Uses Firecrawl for both search and scraping. Good alternative but requires Firecrawl API key.
+- **`playwright_serper`**: Uses Serper.dev for search and Playwright for scraping. More resource-intensive but handles complex websites.
+- **`playwright_ddgs`**: Uses DuckDuckGo for search and Playwright for scraping. Free search but requires browser automation.
+
 Note: If you prefer, you can use DeepSeek instead of OpenAI. You can configure it in the `.env` file by setting the relevant API keys and model. Additionally, ensure that you set `DEFAULT_SERVICE` to `"deepseek"` if using DeepSeek or `"openai"` if using OpenAI.
 
 ## Usage
@@ -132,7 +152,7 @@ cp .env.example .env
 
 # Set your API keys by editing the .env file
 
-# Install playwright dependencies
+# Optional: Install playwright dependencies (only if using playwright_* scrapers)
 playwright install
 
 # Run the tool
@@ -142,17 +162,24 @@ deep-research
 ## Requirements
 
 - Python 3.9 or higher
-- OpenAI API key (GPT-4 access recommended)
-- Firecrawl API key for web search
-- Dependencies:
-  - openai
-  - firecrawl-py
-  - typer
-  - rich
-  - prompt-toolkit
-  - aiohttp
-  - aiofiles
-  - tiktoken
+- OpenAI API key (GPT-4 access recommended) OR DeepSeek API key
+- **For `serper_only` (Recommended)**: Serper.dev API key
+- **For `firecrawl`**: Firecrawl API key  
+- **For Playwright options**: Playwright installation (see below)
+
+### Core Dependencies:
+- openai
+- typer
+- rich
+- prompt-toolkit
+- aiohttp
+- aiofiles
+- tiktoken
+
+### Optional Dependencies (based on scraper choice):
+- firecrawl-py (for `firecrawl` option)
+- playwright (for `playwright_*` options)
+- duckduckgo-search (for `playwright_ddgs` option)
 
 ## Output
 
@@ -168,8 +195,6 @@ MIT
 
 ## Contributing
 
-## Contributing
-
 Contributions are welcome! Please follow these steps:
 
 1. Fork the repository
diff --git a/deep_research_py/data_acquisition/scraper.py b/deep_research_py/data_acquisition/scraper.py
index d80214b..b37dec2 100644
--- a/deep_research_py/data_acquisition/scraper.py
+++ b/deep_research_py/data_acquisition/scraper.py
@@ -4,6 +4,8 @@ from typing import Dict, Any, Optional
 from deep_research_py.utils import logger
 from abc import ABC, abstractmethod
 from playwright.async_api import async_playwright, Browser, BrowserContext, TimeoutError
+import aiohttp
+import os
 
 # ---- Data Models ----
 
@@ -41,6 +43,97 @@ class Scraper(ABC):
         pass
 
 
+class SerperWebpageScraper:
+    """Serper.dev webpage tool scraper implementation."""
+
+    def __init__(self):
+        self.api_key = os.getenv("SERPER_API_KEY")
+        self.api_url = "https://google.serper.dev/webpage"
+        
+        if not self.api_key:
+            raise ValueError("SERPER_API_KEY environment variable is required")
+        
+        logger.info(f"Initialized SerperWebpageScraper with API URL: {self.api_url}")
+
+    async def setup(self):
+        """Initialize the scraper resources."""
+        # No setup needed for API-based scraper
+        logger.info("SerperWebpageScraper setup completed (no resources needed)")
+        pass
+
+    async def teardown(self):
+        """Clean up the scraper resources."""
+        # No cleanup needed for API-based scraper
+        logger.info("SerperWebpageScraper teardown completed (no resources to clean)")
+        pass
+
+    async def scrape(self, url: str, **kwargs) -> ScrapedContent:
+        """Scrape a URL using Serper.dev webpage tool and return standardized content."""
+        logger.info(f"üîç Scraping URL with Serper.dev: {url}")
+        
+        try:
+            headers = {
+                "X-API-KEY": self.api_key,
+                "Content-Type": "application/json"
+            }
+            
+            payload = {
+                "url": url
+            }
+
+            async with aiohttp.ClientSession() as session:
+                async with session.post(
+                    self.api_url, 
+                    json=payload, 
+                    headers=headers
+                ) as response:
+                    if response.status != 200:
+                        logger.error(f"Serper webpage API error for {url}: {response.status}")
+                        return ScrapedContent(
+                            url=url,
+                            html="",
+                            text="",
+                            status_code=response.status,
+                            metadata={"error": f"API error: {response.status}"}
+                        )
+                    
+                    data = await response.json()
+                    logger.debug(f"Serper.dev webpage response keys: {list(data.keys())}")
+
+            # Extract content from Serper response
+            text_content = data.get("text", "")
+            title = data.get("title", "")
+            
+            # Serper doesn't provide HTML, so we'll use the text content
+            html_content = f"<html><head><title>{title}</title></head><body>{text_content}</body></html>"
+            
+            logger.info(f"‚úÖ Successfully scraped {url}: {len(text_content)} characters")
+            logger.debug(f"   Title: {title}")
+            logger.debug(f"   Text length: {len(text_content)} characters")
+            
+            return ScrapedContent(
+                url=url,
+                html=html_content,
+                text=text_content,
+                status_code=200,
+                metadata={
+                    "title": title,
+                    "scraper": "serper_webpage",
+                    "response_data": data
+                }
+            )
+
+        except Exception as e:
+            logger.error(f"‚ùå Error scraping {url} with Serper.dev: {str(e)}")
+            return ScrapedContent(
+                url=url,
+                html="",
+                text="",
+                status_code=0,
+                metadata={"error": str(e), "scraper": "serper_webpage"}
+            )
+
+
 class PlaywrightScraper:
     """Playwright-based scraper implementation."""
 
diff --git a/deep_research_py/data_acquisition/services.py b/deep_research_py/data_acquisition/services.py
index ad16cf9..b6ce98b 100644
--- a/deep_research_py/data_acquisition/services.py
+++ b/deep_research_py/data_acquisition/services.py
@@ -19,6 +19,7 @@ class SearchServiceType(Enum):
     FIRECRAWL = "firecrawl"
     PLAYWRIGHT_DDGS = "playwright_ddgs"
     PLAYWRIGHT_SERPER = "playwright_serper"
+    SERPER_ONLY = "serper_only"
 
 
 class SearchResponse(TypedDict):
@@ -55,6 +56,16 @@ class SearchService:
                 api_url=os.environ.get("FIRECRAWL_BASE_URL"),
             )
             self.manager = None
+        elif service_type == SearchServiceType.SERPER_ONLY.value:
+            logger.info("Using Serper.dev for both search and scraping")
+            # Use Serper for both search and scraping
+            from .scraper import SerperWebpageScraper
+            self.firecrawl = None
+            self.manager = SearchAndScrapeManager(
+                search_engine=SerperSearchEngine(),
+                scraper=SerperWebpageScraper()
+            )
+            self._initialized = False
         elif service_type == SearchServiceType.PLAYWRIGHT_SERPER.value:
             logger.info("Using Serper.dev for search with Playwright for scraping")
             # Use Serper for search with Playwright for scraping
@@ -131,6 +142,10 @@ class SearchService:
                         content_length = len(scraped.text)
                         logger.debug(f"   Content length: {content_length} characters")
                         logger.debug(f"   Status code: {scraped.status_code}")
+                        
+                        # Log scraper type for debugging
+                        scraper_type = scraped.metadata.get("scraper", "unknown") if scraped.metadata else "unknown"
+                        logger.debug(f"   Scraper used: {scraper_type}")
                     else:
                         logger.warning(f"   No scraped content available for: {result.url}")
 
diff --git a/deep_research_py/utils.py b/deep_research_py/utils.py
index 74a2209..c60857a 100644
--- a/deep_research_py/utils.py
+++ b/deep_research_py/utils.py
@@ -16,7 +16,7 @@ def setup_logging(log_level=logging.INFO, log_to_file=True, log_file_path=None):
     """
     
     # Create logs directory if it doesn't exist
-    logs_dir = Path("logs")
+    logs_dir = Path("logs_serper")
     logs_dir.mkdir(exist_ok=True)
     
     # Generate log filename with timestamp if not provided
